{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "TimeoutError",
     "evalue": "[Errno 60] Operation timed out",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mF\u001b[39;00m \n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GitHub/self-learning-AI/.venv/lib/python3.12/site-packages/torch/__init__.py:2059\u001b[0m\n\u001b[1;32m   2043\u001b[0m \u001b[38;5;66;03m################################################################################\u001b[39;00m\n\u001b[1;32m   2044\u001b[0m \u001b[38;5;66;03m# Import most common subpackages\u001b[39;00m\n\u001b[1;32m   2045\u001b[0m \u001b[38;5;66;03m################################################################################\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2050\u001b[0m \n\u001b[1;32m   2051\u001b[0m \u001b[38;5;66;03m# needs to be before import torch.nn as nn to avoid circular dependencies\u001b[39;00m\n\u001b[1;32m   2052\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograd\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# usort: skip\u001b[39;00m\n\u001b[1;32m   2053\u001b[0m     enable_grad \u001b[38;5;28;01mas\u001b[39;00m enable_grad,\n\u001b[1;32m   2054\u001b[0m     inference_mode \u001b[38;5;28;01mas\u001b[39;00m inference_mode,\n\u001b[1;32m   2055\u001b[0m     no_grad \u001b[38;5;28;01mas\u001b[39;00m no_grad,\n\u001b[1;32m   2056\u001b[0m     set_grad_enabled \u001b[38;5;28;01mas\u001b[39;00m set_grad_enabled,\n\u001b[1;32m   2057\u001b[0m )\n\u001b[0;32m-> 2059\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m   2060\u001b[0m     __config__ \u001b[38;5;28;01mas\u001b[39;00m __config__,\n\u001b[1;32m   2061\u001b[0m     __future__ \u001b[38;5;28;01mas\u001b[39;00m __future__,\n\u001b[1;32m   2062\u001b[0m     _awaits \u001b[38;5;28;01mas\u001b[39;00m _awaits,\n\u001b[1;32m   2063\u001b[0m     autograd \u001b[38;5;28;01mas\u001b[39;00m autograd,\n\u001b[1;32m   2064\u001b[0m     backends \u001b[38;5;28;01mas\u001b[39;00m backends,\n\u001b[1;32m   2065\u001b[0m     cpu \u001b[38;5;28;01mas\u001b[39;00m cpu,\n\u001b[1;32m   2066\u001b[0m     cuda \u001b[38;5;28;01mas\u001b[39;00m cuda,\n\u001b[1;32m   2067\u001b[0m     distributed \u001b[38;5;28;01mas\u001b[39;00m distributed,\n\u001b[1;32m   2068\u001b[0m     distributions \u001b[38;5;28;01mas\u001b[39;00m distributions,\n\u001b[1;32m   2069\u001b[0m     fft \u001b[38;5;28;01mas\u001b[39;00m fft,\n\u001b[1;32m   2070\u001b[0m     futures \u001b[38;5;28;01mas\u001b[39;00m futures,\n\u001b[1;32m   2071\u001b[0m     hub \u001b[38;5;28;01mas\u001b[39;00m hub,\n\u001b[1;32m   2072\u001b[0m     jit \u001b[38;5;28;01mas\u001b[39;00m jit,\n\u001b[1;32m   2073\u001b[0m     linalg \u001b[38;5;28;01mas\u001b[39;00m linalg,\n\u001b[1;32m   2074\u001b[0m     mps \u001b[38;5;28;01mas\u001b[39;00m mps,\n\u001b[1;32m   2075\u001b[0m     mtia \u001b[38;5;28;01mas\u001b[39;00m mtia,\n\u001b[1;32m   2076\u001b[0m     multiprocessing \u001b[38;5;28;01mas\u001b[39;00m multiprocessing,\n\u001b[1;32m   2077\u001b[0m     nested \u001b[38;5;28;01mas\u001b[39;00m nested,\n\u001b[1;32m   2078\u001b[0m     nn \u001b[38;5;28;01mas\u001b[39;00m nn,\n\u001b[1;32m   2079\u001b[0m     optim \u001b[38;5;28;01mas\u001b[39;00m optim,\n\u001b[1;32m   2080\u001b[0m     overrides \u001b[38;5;28;01mas\u001b[39;00m overrides,\n\u001b[1;32m   2081\u001b[0m     profiler \u001b[38;5;28;01mas\u001b[39;00m profiler,\n\u001b[1;32m   2082\u001b[0m     sparse \u001b[38;5;28;01mas\u001b[39;00m sparse,\n\u001b[1;32m   2083\u001b[0m     special \u001b[38;5;28;01mas\u001b[39;00m special,\n\u001b[1;32m   2084\u001b[0m     testing \u001b[38;5;28;01mas\u001b[39;00m testing,\n\u001b[1;32m   2085\u001b[0m     types \u001b[38;5;28;01mas\u001b[39;00m types,\n\u001b[1;32m   2086\u001b[0m     utils \u001b[38;5;28;01mas\u001b[39;00m utils,\n\u001b[1;32m   2087\u001b[0m     xpu \u001b[38;5;28;01mas\u001b[39;00m xpu,\n\u001b[1;32m   2088\u001b[0m )\n\u001b[1;32m   2089\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msignal\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m windows \u001b[38;5;28;01mas\u001b[39;00m windows\n\u001b[1;32m   2092\u001b[0m \u001b[38;5;66;03m# Quantized, sparse, AO, etc. should be last to get imported, as nothing\u001b[39;00m\n\u001b[1;32m   2093\u001b[0m \u001b[38;5;66;03m# is expected to depend on them.\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GitHub/self-learning-AI/.venv/lib/python3.12/site-packages/torch/distributions/__init__.py:116\u001b[0m\n",
      "File \u001b[0;32m~/Documents/GitHub/self-learning-AI/.venv/lib/python3.12/site-packages/torch/distributions/von_mises.py:5\u001b[0m\n",
      "File \u001b[0;32m~/Documents/GitHub/self-learning-AI/.venv/lib/python3.12/site-packages/torch/jit/__init__.py:26\u001b[0m\n",
      "File \u001b[0;32m~/Documents/GitHub/self-learning-AI/.venv/lib/python3.12/site-packages/torch/jit/_freeze.py:11\u001b[0m\n",
      "File \u001b[0;32m~/Documents/GitHub/self-learning-AI/.venv/lib/python3.12/site-packages/torch/jit/_script.py:30\u001b[0m\n",
      "File \u001b[0;32m~/Documents/GitHub/self-learning-AI/.venv/lib/python3.12/site-packages/torch/jit/_recursive.py:17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mjit\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_check\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AttributeTypeIsSupportedChecker\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mjit\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_state\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _add_script_class, _get_script_class, _python_cu\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mjit\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfrontend\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     18\u001b[0m     get_class_properties,\n\u001b[1;32m     19\u001b[0m     get_default_args,\n\u001b[1;32m     20\u001b[0m     get_jit_class_def,\n\u001b[1;32m     21\u001b[0m     get_jit_def,\n\u001b[1;32m     22\u001b[0m )\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Module\n\u001b[1;32m     26\u001b[0m ScriptMethodStub \u001b[38;5;241m=\u001b[39m collections\u001b[38;5;241m.\u001b[39mnamedtuple(\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScriptMethodStub\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresolution_callback\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdef_\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moriginal_method\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     28\u001b[0m )\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1354\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1325\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:929\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:990\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:1127\u001b[0m, in \u001b[0;36mget_code\u001b[0;34m(self, fullname)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:1186\u001b[0m, in \u001b[0;36mget_data\u001b[0;34m(self, path)\u001b[0m\n",
      "\u001b[0;31mTimeoutError\u001b[0m: [Errno 60] Operation timed out"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open(\"words_alpha.txt\", \"r\").read().splitlines()\n",
    "\n",
    "words[1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lookup tables\n",
    "\n",
    "characters = sorted(list(set(\"\".join(words))))\n",
    "stoi = {s : i + 1 for i, s in enumerate(characters)}\n",
    "stoi[\".\"] = 0\n",
    "itos = {i : s for s, i in stoi.items()}\n",
    "\n",
    "vocab_size = 27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building the dataset\n",
    "random.seed(2)\n",
    "block_size = 3 # how many chars used to predict the next character\n",
    "\n",
    "def build_dataset(words):\n",
    "    X, y = [], []\n",
    "\n",
    "    for w in words:\n",
    "        context = [0] * block_size # start characters in the start\n",
    "        for ch in w + '.':\n",
    "            ix = stoi[ch] \n",
    "            X.append(context)\n",
    "            y.append(ix)\n",
    "            context = context[1:] + [ix] # crop and append \n",
    "\n",
    "    X = torch.tensor(X)\n",
    "    y = torch.tensor(y)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8 * len(words))\n",
    "n2 = int(0.9 * len(words))\n",
    "\n",
    "X_train, y_train = build_dataset(words[:n1])\n",
    "X_dev, y_dev = build_dataset(words[n1:n2])\n",
    "X_test, y_test = build_dataset(words[n2:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants for neural net training \n",
    "n_dims = 10\n",
    "n_hidden = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch-like API for neural net layers\n",
    "\n",
    "class Linear:\n",
    "    def __init__(self, num_in, num_out, bias = True):\n",
    "        self.weight = torch.randn((num_in, num_out)) / (num_in ** (1/2))\n",
    "        self.bias = torch.zeros(num_out) if bias else None\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        self.out = x @ self.weight\n",
    "        if self.bias is not None:\n",
    "            self.out += self.bias\n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.weight] + ([] if self.bias is None else [self.bias])\n",
    "    \n",
    "class BatchNorm1D:\n",
    "    def __init__(self, dim, eps = 1e-5, momentum = 0.1):\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        self.training = True\n",
    "        self.gamma = torch.ones(dim)\n",
    "        self.beta = torch.zeros(dim)\n",
    "        self.rmean = torch.zeros(dim)\n",
    "        self.rvar = torch.ones(dim)\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        if self.training:\n",
    "            xmean = x.mean(0, keepdim = True)\n",
    "            xvar = x.var(0, keepdim = True)\n",
    "        else:\n",
    "            xmean = self.rmean\n",
    "            xvar = self.var\n",
    "        xhat = (x - xmean) / torch.sqrt(xvar + self.eps)\n",
    "        self.out = self.gamma * xhat + self.beta\n",
    "        \n",
    "        if self.training:\n",
    "            with torch.no_grad():\n",
    "                self.rmean = (1 - self.momentum) * self.rmean + self.momentum * xmean\n",
    "                self.rvar = (1 - self.momentum) * self.rvar + self.momentum * xvar\n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta]\n",
    "    \n",
    "class Tanh:\n",
    "    def __call__(self, x):\n",
    "        self.out = torch.tanh(x)\n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return []            \n",
    "    \n",
    "# class Embeddings:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = torch.randn((vocab_size, n_dims))\n",
    "layers = [\n",
    "    Linear(n_dims * block_size, n_hidden), BatchNorm1D(n_hidden), Tanh(),\n",
    "    Linear(           n_hidden, n_hidden), BatchNorm1D(n_hidden), Tanh(),\n",
    "    Linear(           n_hidden, n_hidden), BatchNorm1D(n_hidden), Tanh(),\n",
    "    Linear(           n_hidden, n_hidden), BatchNorm1D(n_hidden), Tanh(),\n",
    "    Linear(           n_hidden, n_hidden), BatchNorm1D(n_hidden), Tanh(),\n",
    "    Linear(           n_hidden, vocab_size), BatchNorm1D(vocab_size), \n",
    "]\n",
    "\n",
    "with torch.no_grad():\n",
    "    # make last layer less confident\n",
    "    layers[-1].gamma *= 0.1\n",
    "    # apply gain on all remaining layers\n",
    "    for layer in layers[:-1]:\n",
    "        if isinstance(layer, Linear):\n",
    "            layer.weight *= 5/3\n",
    "    \n",
    "parameters = [C] + [p for layer in layers for p in layer.parameters()]\n",
    "print(sum(p.nelement() for p in parameters))\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "\n",
    "for i in range(max_steps):\n",
    "    ix = torch.randint(0, X_train.shape[0], (batch_size, ))\n",
    "    Xb, yb = X_train[ix], y_train[ix]\n",
    "    \n",
    "    # forward\n",
    "    emb = C[Xb]\n",
    "    x = emb.view(emb.shape[0], -1)\n",
    "    for layer in layers:\n",
    "        x = layer(x)\n",
    "    loss = F.cross_entropy(x, yb)\n",
    "    \n",
    "    # backward\n",
    "    for layer in layers:\n",
    "        layer.out.retain_grad()\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    \n",
    "    #update\n",
    "    lr = 0.1 if i < max_steps / 2 else 0.01\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "    \n",
    "    # tracking stats\n",
    "    print(f'Run {i}/{max_steps - 1}, Loss: {loss}') if (((i % (max_steps / 10)) == 0) or (i == max_steps - 1)) else \"\"\n",
    "    lossi.append(loss.log10().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
